{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Flow\n",
    "\n",
    "Before proceeding with the algorithm, let’s first discuss the lifecycle of any machine learning model. This diagram explains the creation of a Machine Learning model from scratch and then taking the same model further with hyperparameter tuning to increase its accuracy, deciding the deployment strategies for that model and once deployed setting up the logging and monitoring frameworks to generate reports and dashboards based on the client requirements. \n",
    "A typical lifecycle diagram for a machine learning model looks like:\n",
    "\n",
    "<img src=\"MLApplicationFlow_bold.PNG\" width= \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Boosting is an ensemble approach(meaning it involves several trees) that starts from a weaker decision and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers.\n",
    "The weights are assigned based on the performance of an individual tree.\n",
    "\n",
    "<img src= \"boosting_basic.png\" alt='boosting' style=\"width: 400px;\">\n",
    "\n",
    "\n",
    "Ensemble parameters are calculated in **stagewise way** which means that while calculating the subsequent weight, the learning from the previous tree is considered as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak classifier - why tree?\n",
    "First what is a weak classifier?\n",
    "**Weak classifier** -  *slightly better* than random guessing.\n",
    "\n",
    "Any algorithm could have been used as a base for the boosting technique, but the reason for choosing trees are:\n",
    "\n",
    "#### Pro's\n",
    "- computational scalability,\n",
    "- handles missing values,\n",
    "- robust to outliers,\n",
    "- does not require feature scaling,\n",
    "- can deal with irrelevant inputs,\n",
    "- interpretable (if small),\n",
    "- handles mixed predictors as well (quantitive and qualitative)\n",
    "\n",
    "#### Con's\n",
    "- inability to extract a linear combination of features\n",
    "- high variance leading to a small computational power\n",
    "\n",
    "And that’s where boosting comes into the picture. It minimises the variance by taking into consideration the results from various trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every machine learning model, the training objective is a sum of a loss function $L$ and regularisation $\\Omega$:\n",
    "\n",
    "$$\n",
    "obj = L + \\Omega\n",
    "$$\n",
    "\n",
    "The loss function controls the predictive power of an algorithm and the regularisation term controls its simplicity.\n",
    "\n",
    "There are several algorithms which use boosting. A few are discussed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost (Adaptive Boosting)\n",
    "The steps to implement the Ada Boost algorithm using the decision trees are as follows:\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "Assume that the number of training samples is denoted by $N$, and the number of iterations (created trees) is $M$. Notice that possible class outputs are $Y=\\{-1,1\\}$\n",
    "\n",
    "1. Initialize the observation weights  $w_i=\\frac{1}{N}$ where $i = 1,2, \\dots, N$ for all the samples.\n",
    "2. For $m=1$ to $M$:\n",
    "    - fit a classifier $G_m(x)$ to the training data using weights $w_i$,\n",
    "    - compute $err_m = \\frac{\\sum_{i=1}^{N} w_i I (y_i \\neq G_m(x))}{\\sum_{i=1}^{N}w_i}$,\n",
    "    - compute $\\alpha_m = \\frac {1}{2} \\log (\\frac{(1-err_m)}{err_m})$. This is the contribution of that tree to the final result.\n",
    "    - calculate the new weights using the formula:\n",
    "    \n",
    "    $w_i \\leftarrow w_i \\cdot \\exp [\\alpha_m \\cdot I (y_i \\neq G_m(x)]$, where $i = 1,2, \\dots, N$\n",
    "- Normalize the new sample  weights so that their sum is 1.\n",
    "- Construct the next tree using the new weights\n",
    "\n",
    "\n",
    " 3. At the end, compare the summation of results from all the trees and the final result is either the one with the highest sum(for regression) or it is the class which has the most weighted voted average(for classification).\n",
    "\n",
    "       Output $G_m(x) = argmax [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Regression)\n",
    "\n",
    "       Output $G_m(x) = sigm [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "For understanding this algorithm, we'll use the following simple dataset for heart  patient prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is Chest Pain Present</th>\n",
       "      <th>Are any arteries blocked</th>\n",
       "      <th>Weight of the person</th>\n",
       "      <th>Is Heart Patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>205</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>180</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>210</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>167</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>156</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>125</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>168</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>172</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Is Chest Pain Present Are any arteries blocked  Weight of the person  \\\n",
       "0                   YES                      YES                   205   \n",
       "1                    NO                      YES                   180   \n",
       "2                   YES                       NO                   210   \n",
       "3                   YES                      YES                   167   \n",
       "4                    NO                      YES                   156   \n",
       "5                    NO                      YES                   125   \n",
       "6                   YES                       NO                   168   \n",
       "7                   YES                      YES                   172   \n",
       "\n",
       "  Is Heart Patient  \n",
       "0              YES  \n",
       "1              YES  \n",
       "2              YES  \n",
       "3              YES  \n",
       "4               NO  \n",
       "5               NO  \n",
       "6               NO  \n",
       "7               NO  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "heart_data= pd.read_csv('heart_disease.csv')\n",
    "heart_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a total of 8 rows in our dataset. Hence, we’ll initialize the sample weights($w=\\frac {1}{N}$) as 1/8 in the beginning. And, at the beginning, all the samples are equally important.\n",
    "\n",
    "<img src='sw1.PNG' width=”500”>\n",
    "\n",
    "- We’ll consider the individual columns to create weak decision-makers as shown below and then try to figure out what are the correct and incorrect predictions based on that column.\n",
    "\n",
    "<img src='cp1.PNG' width=”200”>\n",
    "\n",
    "<img src='ba1.PNG' width=”200”>\n",
    "\n",
    "<img src='bw1.PNG' width=”200”>\n",
    "\n",
    "- We’ll now calculate the Gini index of the individual stumps using the formula\n",
    "\n",
    "     G.I= $\\sum (weight of the decision)*(1-(p^2+(1-p)^2))$\n",
    "\n",
    "        G.I for chest pain tree= 0.47\n",
    "        G.I for blocked arteries tree= 0.5\n",
    "        G.I for body-weight tree= 0.2\n",
    "        \n",
    "        And, we select the tree with the lowest Gini Index. This will be the first decision-maker for our model.\n",
    "\n",
    "- Now, we’ll calculate the contribution of this tree(stump) to our final decision using the formula:\n",
    "\n",
    "Contribution= $½(log (1-total error)/total error)$\n",
    "\n",
    "    As this stump classified only one data incorrectly out of the 8, hence the total error is 1/8.\n",
    "\n",
    "    Putting this into the formula we get contribution= 0.97\n",
    "    \n",
    "- We’ll now calculate the new weights using the formula:\n",
    "\n",
    "1. Increase the sample weight for incorrectly classified datapoints\n",
    "    New weight= old weight*e^ contribution= 1/8* e^0.97=0.33\n",
    "1. Decrease the sample weight for incorrectly classified datapoints\n",
    "   New weight= old weight*e^- contribution= 1/8* e^-0.97=0.05\n",
    "\n",
    "- Populate the new weights as shown below:\n",
    "\n",
    "     <img src='nsw1.PNG' width=”300”>\n",
    "\n",
    "- Normalize the sample weights: If we add all the new sample weights, we get 0.68. Hence, for normalization we divide all the sample weights by 0.68 and then create normalized sample weights as shown below: \n",
    "\n",
    "     <img src='normalized_wt.PNG' width=”200”>\n",
    "\n",
    "       These new normalized weights will act as the sample weights for the next iteration.\n",
    "\n",
    "- Then we create new trees which consider the dataset which was prepared using the new sample weights.\n",
    "\n",
    "- Suppose, m trees(stumps) are classifying a person as a heart patient and n trees(stumps) are classifying a person as a healthy one, then the contribution of m and n trees are added separately and whichever has the higher value, the person is classified as that. \n",
    "\n",
    "_For example, if the contribution of m trees is 1.2 and the contribution of n trees is 0.5 then the final result will go in the favour of m trees and the person will be classified as a heart patient._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees\n",
    "*Gradient Boosted Trees* use decision trees as estimators. It can work with different loss functions (regression, classification, risk modelling etc.), evaluate it's gradient and approximates it with a simple tree (stage-wisely, that minimizes the overall error).\n",
    "\n",
    "AdaBoost is a special case of Gradient Boosted Tree that uses exponential loss function.\n",
    "\n",
    "**The Algorithm:**\n",
    "\n",
    "- Calculate the average of the label column as initially this average shall minimise the total error.\n",
    "- Calculate the pseudo residuals.\n",
    "       Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    "  Mathematically,\n",
    "  \n",
    "     derivative of the pseudo residual=$(\\frac {\\delta L(y_i,f(x_i))}{\\delta (f(x_i))})$\n",
    "     \n",
    "     where, L is the loss function.\n",
    "                          \n",
    "               \n",
    "     Here, the gradient of the error term is getting calculated as the goal is to minimize the error. Hence the name gradient boosted trees\n",
    "- create a tree to predict the pseudo residuals instead  of a tree to predict for the actual column values.\n",
    "- new result= previous result+learning rate* residual \n",
    "   \n",
    "   Mathematically, \n",
    "     $ F_1(x)= F_0(x)+ \\nu \\sum \\gamma $\n",
    "     \n",
    " where  $ \\nu $ is the learning rate and $ \\gamma $ is the residual\n",
    "\n",
    "Repeat these steps until the residual stops decreasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "For understanding this algorithm we'll use the following simple dataset for weight prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person Height(in metres)</th>\n",
       "      <th>Person Favorite Colour</th>\n",
       "      <th>Person Gender</th>\n",
       "      <th>Person Weight (in Kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.6</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Male</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.6</td>\n",
       "      <td>Green</td>\n",
       "      <td>Female</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Female</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.8</td>\n",
       "      <td>Red</td>\n",
       "      <td>Male</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>Green</td>\n",
       "      <td>Male</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.4</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Female</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Person Height(in metres) Person Favorite Colour Person Gender  \\\n",
       "0                       1.6                   Blue          Male   \n",
       "1                       1.6                  Green        Female   \n",
       "2                       1.5                   Blue        Female   \n",
       "3                       1.8                    Red          Male   \n",
       "4                       1.5                  Green          Male   \n",
       "5                       1.4                   Blue        Female   \n",
       "\n",
       "   Person Weight (in Kg)  \n",
       "0                     88  \n",
       "1                     76  \n",
       "2                     56  \n",
       "3                     73  \n",
       "4                     77  \n",
       "5                     57  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "weight_data= pd.read_csv('weights.csv')\n",
    "weight_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the first iteration, calculate the average of the target column(weight here) as it minimizes the residual initially.\n",
    "\n",
    "Average=(88+76+56+73+77+57)/6=  71.2\n",
    "\n",
    "- We consider this as the first prediction and then we’ll calculate the residual which is the difference between the predicted and the actual value as shown below:\n",
    "\n",
    "<img src='pseudo_residuals1.PNG' width=”300”>\n",
    "- Now we build a tree to predict the residuals as shown below:\n",
    "<img src='residual_tree1.PNG' width=”300”>\n",
    "\n",
    "We are only building here till a limited depth just for simplicity. As you can see, some leaves have more than one residuals. For those, we’ll calculate the average and the final tree will look like:\n",
    "<img src='average_residual_tree.PNG' width=”300”>\n",
    "- Now for prediction, we use the formula\n",
    "\n",
    "                        New value= old value+learning rate * residual\n",
    "If we consider the learning rate as 0.1, the result becomes.\n",
    "\n",
    "                        New value=  71.2+0.1*16.8= 72.9 (for the first row).\n",
    "    Similarly the new predictions for all the rows is calculated.\n",
    "                        \n",
    "- The above steps are repeated until there is no significant improvement in residuals.\n",
    "- The final result is given by\n",
    "                \n",
    "                Final Value= First Prediction+learning rate* 1st residual+ learning rate* 2nd residual+ and so on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "XGBoost improves the gradient boosting method even further.\n",
    "> **XGBoost** (*extreme gradient boosting*) regularises data better than normal gradient boosted Trees.\n",
    "\n",
    "It was developed by Tianqi Chen in C++ but now has interfaces for Python, R, Julia.\n",
    "\n",
    "XGBoost's objective function is the sum of loss function evaluated over all the predictions and a regularisation function for all predictors ($j$ trees). In the formula $f_j$ means a prediction coming from the $j^th$ tree.\n",
    "\n",
    "$$\n",
    "obj(\\theta) = \\sum_{i}^{n} l(y_i - \\hat{y_i}) +  \\sum_{j=1}^{j} \\Omega (f_j)\n",
    "$$\n",
    "\n",
    "Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n",
    "$$\n",
    "\n",
    "First part ($\\gamma T$) is responsible for controlling the overall number of created leaves, and the second term ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) watches over the scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematics Involved**\n",
    "Unlike the other tree-building algorithms, XGBoost doesn’t use entropy or Gini indices. Instead, it utilises gradient (the error term) and hessian for creating the trees. Hessian for a Regression problem is the *number of residuals* and for a classification problem. Mathematically, Hessian is a second order derivative of the loss at the current estimate given as:\n",
    "\n",
    "<img src=\"hessian.PNG\" width=\"300\">\n",
    "\n",
    "where **L** is the loss function. \n",
    "\n",
    "- Initialise the tree with only one leaf.\n",
    "- compute the similarity using the formula\n",
    "$$\n",
    "Similarity= \\frac {Gradient^2}{ hessian +\\lambda}\n",
    "$$\n",
    "Where $\\lambda $ is the regularisation term.\n",
    "- Now for splitting data into a tree form, calculate\n",
    "$$\n",
    "Gain=  left similarity+right similarity-similarity for root\n",
    "$$ \n",
    "- For tree pruning, the parameter $ \\gamma$ is used. The algorithm starts from the lowest level of the tree and then starts pruning based on the value of $\\gamma$.\n",
    "\n",
    "\n",
    " If $Gain- \\gamma < 0$, remove that branch. Else, keep the branch \n",
    " \n",
    "- Learning is done using the equation\n",
    "$$\n",
    "New Value= old Value+ \\eta * prediction\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**:\n",
    "The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n",
    "It is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values. The variable names are as follows:\n",
    "1.\tNumber of times pregnant.\n",
    "2.\tPlasma glucose concentration 2 hours in an oral glucose tolerance test.\n",
    "3.\tDiastolic blood pressure (mm Hg).\n",
    "4.\tTriceps skinfold thickness (mm).\n",
    "5.\t2-Hour serum insulin (mu U/ml).\n",
    "6.\tBody mass index (weight in kg/(height in m)^2).\n",
    "7.\tDiabetes pedigree function.\n",
    "8.\tAge (years).\n",
    "9.\tIs Diabetic (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn import datasets\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the features and the labels\n",
    "data= pd.read_csv('pima-indians-diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>Plasma glucose concentration</th>\n",
       "      <th>Diastolic blood pressure (mm Hg)</th>\n",
       "      <th>Triceps skinfold thickness (mm)</th>\n",
       "      <th>2-Hour serum insulin (mu U/ml)</th>\n",
       "      <th>Body mass index (weight in kg/(height in m)^2)</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age</th>\n",
       "      <th>Is Diabetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of times pregnant  Plasma glucose concentration  \\\n",
       "0                         6                           148   \n",
       "1                         1                            85   \n",
       "2                         8                           183   \n",
       "3                         1                            89   \n",
       "4                         0                           137   \n",
       "\n",
       "   Diastolic blood pressure (mm Hg)  Triceps skinfold thickness (mm)  \\\n",
       "0                                72                               35   \n",
       "1                                66                               29   \n",
       "2                                64                                0   \n",
       "3                                66                               23   \n",
       "4                                40                               35   \n",
       "\n",
       "   2-Hour serum insulin (mu U/ml)  \\\n",
       "0                               0   \n",
       "1                               0   \n",
       "2                               0   \n",
       "3                              94   \n",
       "4                             168   \n",
       "\n",
       "   Body mass index (weight in kg/(height in m)^2)  Diabetes pedigree function  \\\n",
       "0                                            33.6                       0.627   \n",
       "1                                            26.6                       0.351   \n",
       "2                                            23.3                       0.672   \n",
       "3                                            28.1                       0.167   \n",
       "4                                            43.1                       2.288   \n",
       "\n",
       "   Age  Is Diabetic  \n",
       "0   50            1  \n",
       "1   31            0  \n",
       "2   32            1  \n",
       "3   21            0  \n",
       "4   33            1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Number of times pregnant', 'Plasma glucose concentration',\n",
       "       'Diastolic blood pressure (mm Hg)', 'Triceps skinfold thickness (mm)',\n",
       "       '2-Hour serum insulin (mu U/ml)',\n",
       "       'Body mass index (weight in kg/(height in m)^2)',\n",
       "       'Diabetes pedigree function', 'Age', 'Is Diabetic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Plasma glucose concentration',\n",
    "       'Diastolic blood pressure (mm Hg)', 'Triceps skinfold thickness (mm)',\n",
    "       '2-Hour serum insulin (mu U/ml)',\n",
    "       'Body mass index (weight in kg/(height in m)^2)',\n",
    "       'Diabetes pedigree function', 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as mentioned in the data description, the missing values have been replaced by zeroes. So, we are replacing zeroes with nan\n",
    "for col in cols:\n",
    "    data[col]=data[col].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of times pregnant                            0\n",
       "Plasma glucose concentration                        5\n",
       "Diastolic blood pressure (mm Hg)                   35\n",
       "Triceps skinfold thickness (mm)                   227\n",
       "2-Hour serum insulin (mu U/ml)                    374\n",
       "Body mass index (weight in kg/(height in m)^2)     11\n",
       "Diabetes pedigree function                          0\n",
       "Age                                                 0\n",
       "Is Diabetic                                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the missing values\n",
    "data['Plasma glucose concentration']=data['Plasma glucose concentration'].fillna(data['Plasma glucose concentration'].mode()[0])\n",
    "data['Diastolic blood pressure (mm Hg)']=data['Diastolic blood pressure (mm Hg)'].fillna(data['Diastolic blood pressure (mm Hg)'].mode()[0])\n",
    "data['Triceps skinfold thickness (mm)']=data['Triceps skinfold thickness (mm)'].fillna(data['Triceps skinfold thickness (mm)'].mean())\n",
    "data['2-Hour serum insulin (mu U/ml)']=data['2-Hour serum insulin (mu U/ml)'].fillna(data['2-Hour serum insulin (mu U/ml)'].mean())\n",
    "data['Body mass index (weight in kg/(height in m)^2)']=data['Body mass index (weight in kg/(height in m)^2)'].fillna(data['Body mass index (weight in kg/(height in m)^2)'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of times pregnant                          0\n",
       "Plasma glucose concentration                      0\n",
       "Diastolic blood pressure (mm Hg)                  0\n",
       "Triceps skinfold thickness (mm)                   0\n",
       "2-Hour serum insulin (mu U/ml)                    0\n",
       "Body mass index (weight in kg/(height in m)^2)    0\n",
       "Diabetes pedigree function                        0\n",
       "Age                                               0\n",
       "Is Diabetic                                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values after imputation\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the feature and the Label columns \n",
    "x=data.drop(labels='Is Diabetic', axis=1)\n",
    "y= data['Is Diabetic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>Plasma glucose concentration</th>\n",
       "      <th>Diastolic blood pressure (mm Hg)</th>\n",
       "      <th>Triceps skinfold thickness (mm)</th>\n",
       "      <th>2-Hour serum insulin (mu U/ml)</th>\n",
       "      <th>Body mass index (weight in kg/(height in m)^2)</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of times pregnant  Plasma glucose concentration  \\\n",
       "0                         6                         148.0   \n",
       "1                         1                          85.0   \n",
       "2                         8                         183.0   \n",
       "3                         1                          89.0   \n",
       "4                         0                         137.0   \n",
       "\n",
       "   Diastolic blood pressure (mm Hg)  Triceps skinfold thickness (mm)  \\\n",
       "0                              72.0                         35.00000   \n",
       "1                              66.0                         29.00000   \n",
       "2                              64.0                         29.15342   \n",
       "3                              66.0                         23.00000   \n",
       "4                              40.0                         35.00000   \n",
       "\n",
       "   2-Hour serum insulin (mu U/ml)  \\\n",
       "0                      155.548223   \n",
       "1                      155.548223   \n",
       "2                      155.548223   \n",
       "3                       94.000000   \n",
       "4                      168.000000   \n",
       "\n",
       "   Body mass index (weight in kg/(height in m)^2)  Diabetes pedigree function  \\\n",
       "0                                            33.6                       0.627   \n",
       "1                                            26.6                       0.351   \n",
       "2                                            23.3                       0.672   \n",
       "3                                            28.1                       0.167   \n",
       "4                                            43.1                       2.288   \n",
       "\n",
       "   Age  \n",
       "0   50  \n",
       "1   31  \n",
       "2   32  \n",
       "3   21  \n",
       "4   33  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\virat\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\virat\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# as the datapoints differ a lot in magnitude, we'll scale them\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaled_data=scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x,test_x,train_y,test_y=train_test_split(scaled_data,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier(objective='binary:logistic')\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050279329608939"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheking training accuracy\n",
    "y_pred = model.predict(train_x)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(train_y,predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7402597402597403"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheking initial test accuracy\n",
    "y_pred = model.predict(test_x)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(test_y,predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63994726, -0.77251205, -1.18156252,  0.43784695,  0.40547846,\n",
       "        0.22451019, -0.1264714 ,  0.83038113])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to increae the accuracy of the model, we'll do hyperparameter tuning using grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\n",
    "   \n",
    "    ' learning_rate':[1,0.5,0.1,0.01,0.001],\n",
    "    'max_depth': [3,5,10,20],\n",
    "    'n_estimators':[10,50,100,200]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid= GridSearchCV(XGBClassifier(objective='binary:logistic'),param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\virat\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=10 ..................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=10, score=0.7777777777777778, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=10 ..................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=10, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=10 ..................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=50 ..................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=50, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=50 ..................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=50, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=50 ..................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=100 .................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=100, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=100 .................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=100, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=100 .................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=200 .................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=200, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=200 .................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=200, score=0.7653631284916201, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=3, n_estimators=200 .................\n",
      "[CV]   learning_rate=1, max_depth=3, n_estimators=200, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=10 ..................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=10, score=0.7833333333333333, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=10 ..................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=10, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=10 ..................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=10, score=0.7247191011235955, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=50 ..................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=50, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=50 ..................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=50, score=0.7877094972067039, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=50 ..................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=50, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=100 .................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=100, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=100 .................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=100, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=100 .................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=200 .................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=200, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=200 .................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=200, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=5, n_estimators=200 .................\n",
      "[CV]   learning_rate=1, max_depth=5, n_estimators=200, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=10 .................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=10, score=0.7333333333333333, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=10 .................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=10 .................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=50 .................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=50 .................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=50, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=50 .................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=50, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=100 ................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=100, score=0.75, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=100 ................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=100, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=100 ................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=200 ................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=200, score=0.75, total=   0.1s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=200 ................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=200, score=0.7318435754189944, total=   0.1s\n",
      "[CV]  learning_rate=1, max_depth=10, n_estimators=200 ................\n",
      "[CV]   learning_rate=1, max_depth=10, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=10 .................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=10, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=10 .................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=10 .................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=50 .................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=50 .................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=50, score=0.7821229050279329, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=50 .................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=100 ................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=100, score=0.7611111111111111, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=100 ................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=100, score=0.7541899441340782, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=100 ................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=200 ................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=200, score=0.7666666666666667, total=   0.1s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=200 ................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=200, score=0.7486033519553073, total=   0.1s\n",
      "[CV]  learning_rate=1, max_depth=20, n_estimators=200 ................\n",
      "[CV]   learning_rate=1, max_depth=20, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=10, score=0.7777777777777778, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=10, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=10 ................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=50, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=50, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=100, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=100, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=200, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=200, score=0.7653631284916201, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=3, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=3, n_estimators=200, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=10, score=0.7833333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=10, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=10, score=0.7247191011235955, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=50, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=50, score=0.7877094972067039, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=50, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=100, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=100, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=200, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=200, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=5, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=5, n_estimators=200, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=10, score=0.7333333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=50, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=50, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=100, score=0.75, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=100, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=200, score=0.75, total=   0.1s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=200, score=0.7318435754189944, total=   0.1s\n",
      "[CV]  learning_rate=0.5, max_depth=10, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=10, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=10, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=50, score=0.7821229050279329, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=100, score=0.7611111111111111, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=100, score=0.7541899441340782, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=200, score=0.7666666666666667, total=   0.1s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=200, score=0.7486033519553073, total=   0.1s\n",
      "[CV]  learning_rate=0.5, max_depth=20, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.5, max_depth=20, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=10, score=0.7777777777777778, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=10, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=50, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=50, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=100, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=100, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=200, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=200, score=0.7653631284916201, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=3, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=3, n_estimators=200, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=10, score=0.7833333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=10, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=10 ................\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=10, score=0.7247191011235955, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=50, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=50, score=0.7877094972067039, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=50 ................\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=50, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=100, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=100, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=100 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=200, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=200, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=5, n_estimators=200 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=5, n_estimators=200, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=10, score=0.7333333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=50, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=50, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=100, score=0.75, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=100, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=200, score=0.75, total=   0.1s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=200, score=0.7318435754189944, total=   0.1s\n",
      "[CV]  learning_rate=0.1, max_depth=10, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=10, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=10, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=50, score=0.7821229050279329, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=100, score=0.7611111111111111, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=100, score=0.7541899441340782, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=200, score=0.7666666666666667, total=   0.1s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=200, score=0.7486033519553073, total=   0.1s\n",
      "[CV]  learning_rate=0.1, max_depth=20, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.1, max_depth=20, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=10, score=0.7777777777777778, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=10, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=50, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=50, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=100, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=100, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=200, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=200, score=0.7653631284916201, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=3, n_estimators=200 ..............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]   learning_rate=0.01, max_depth=3, n_estimators=200, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=10, score=0.7833333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=10, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=10 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=10, score=0.7247191011235955, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=50, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=50, score=0.7877094972067039, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=50 ...............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=50, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=100, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=100, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=100 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=200, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=200, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=5, n_estimators=200 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=5, n_estimators=200, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=10, score=0.7333333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=50, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=50, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=100, score=0.75, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=100, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=200, score=0.75, total=   0.1s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=200, score=0.7318435754189944, total=   0.1s\n",
      "[CV]  learning_rate=0.01, max_depth=10, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.01, max_depth=10, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=10, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=50, score=0.7821229050279329, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=100, score=0.7611111111111111, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=100, score=0.7541899441340782, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=200, score=0.7666666666666667, total=   0.1s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=200, score=0.7486033519553073, total=   0.1s\n",
      "[CV]  learning_rate=0.01, max_depth=20, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.01, max_depth=20, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=10, score=0.7777777777777778, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=10, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=50, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=50, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=100, score=0.7888888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=100, score=0.776536312849162, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=200, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=200, score=0.7653631284916201, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=3, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.001, max_depth=3, n_estimators=200, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=10, score=0.7833333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=10, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=10 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=10, score=0.7247191011235955, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=50, score=0.7666666666666667, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=50, score=0.7877094972067039, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=50 ..............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=50, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=100, score=0.7666666666666667, total=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=100, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=100 .............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=200, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=200, score=0.7486033519553073, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=5, n_estimators=200 .............\n",
      "[CV]   learning_rate=0.001, max_depth=5, n_estimators=200, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=10 .............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=10, score=0.7333333333333333, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=10 .............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=10 .............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=50 .............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=50 .............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=50, score=0.770949720670391, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=50 .............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=50, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=100 ............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=100, score=0.75, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=100 ............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=100, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=100 ............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=100, score=0.7865168539325843, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=200 ............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=200, score=0.75, total=   0.1s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=200 ............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=200, score=0.7318435754189944, total=   0.1s\n",
      "[CV]  learning_rate=0.001, max_depth=10, n_estimators=200 ............\n",
      "[CV]   learning_rate=0.001, max_depth=10, n_estimators=200, score=0.7752808988764045, total=   0.1s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=10 .............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=10, score=0.7388888888888889, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=10 .............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=10, score=0.7597765363128491, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=10 .............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=10, score=0.7303370786516854, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=50 .............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=50, score=0.7555555555555555, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=50 .............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=50, score=0.7821229050279329, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=50 .............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=50, score=0.7584269662921348, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=100 ............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=100, score=0.7611111111111111, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=100 ............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=100, score=0.7541899441340782, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=100 ............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=100, score=0.7640449438202247, total=   0.0s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=200 ............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=200, score=0.7666666666666667, total=   0.1s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=200 ............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=200, score=0.7486033519553073, total=   0.1s\n",
      "[CV]  learning_rate=0.001, max_depth=20, n_estimators=200 ............\n",
      "[CV]   learning_rate=0.001, max_depth=20, n_estimators=200, score=0.7752808988764045, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   15.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={' learning_rate': [1, 0.5, 0.1, 0.01, 0.001], 'max_depth': [3, 5, 10, 20], 'n_estimators': [10, 50, 100, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To  find the parameters givingmaximum accuracy\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=1,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=50, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new model using the same parameters\n",
    "new_model=XGBClassifier(learning_rate= 1, max_depth= 5, n_estimators= 50)\n",
    "new_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7445887445887446"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_new = new_model.predict(test_x)\n",
    "predictions_new = [round(value) for value in y_pred_new]\n",
    "accuracy_new = accuracy_score(test_y,predictions_new)\n",
    "accuracy_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have increased the accuracy of the model, we'll save this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'xgboost_model.pickle'\n",
    "pickle.dump(new_model, open(filename, 'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll save the scaler object as well for prediction\n",
    "filename_scaler = 'scaler_model.pickle'\n",
    "pickle.dump(scaler, open(filename_scaler, 'wb'))\n",
    "\n",
    "scaler_model = pickle.load(open(filename_scaler, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data belongs to class : 1\n"
     ]
    }
   ],
   "source": [
    "# Trying a random prediction\n",
    "d=scaler_model.transform([[6,148,72,35,80,33.6,0.627,50]])\n",
    "pred=loaded_model.predict(d)\n",
    "print('This data belongs to class :',pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main advantages:**\n",
    "- out of the box feature of appropriate bias-variance trade-off,\n",
    "- great computation speed as it utilises parallel computing and cache optimization,\n",
    "- uses hardware optimization,\n",
    "- works well even if the features are correlated\n",
    "- robust even if there is noise for classification problem\n",
    "- the facility of early stopping\n",
    "- the package is evolving, i.e., new features are being added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Deployment\n",
    "\n",
    "Once the training is completed, we need to expose the trained model as an API for the user to consume it. For prediction, the saved model is loaded first and then the predictions are done using it. If the web app works fine, the same app is deployed to the cloud platform.\n",
    "The flow for that can be shown as:\n",
    "\n",
    "<img src=\"testing_pipeline.PNG\">\n",
    "\n",
    "We'll deploy this model to the Google Cloud Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites for Cloud Deployment:**\n",
    "* Basic knowledge of flask framework.\n",
    "* Any Python IDE installed(we are using PyCharm).\n",
    "* A Google Cloud Platform account.\n",
    "* Basic understanding of HTML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Flask App**\n",
    "As we’ll expose the created model as a web API to be consumed by the client/client APIs, we’d do it using the flask framework. \n",
    "\n",
    "* Create the project structure, as shown below:\n",
    "<img src=\"project_structure.PNG\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of **main.py** are:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# importing the necessary dependencies\n",
    "from flask import Flask, render_template, request,jsonify\n",
    "from flask_cors import CORS,cross_origin\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__) # initializing a flask app\n",
    "\n",
    "\n",
    "@app.route('/predict',methods=['POST','GET']) # route to show the predictions in a web UI\n",
    "@cross_origin()\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            #  reading the inputs given by the user\n",
    "            number_of_times_pregnant=float(request.json['number_of_times_pregnant'])\n",
    "            plasma_glucose_concentration = float(request.json['plasma_glucose_concentration'])\n",
    "            diastolic_blood_pressure = float(request.json['diastolic_blood_pressure'])\n",
    "            triceps_skinfold_thickness  = float(request.json['triceps_skinfold_thickness'])\n",
    "            serum_insulin  = float(request.json['serum_insulin'])\n",
    "            body_mass_index  = float(request.json['body_mass_index'])\n",
    "            diabetes_pedigree_function = float(request.json['diabetes_pedigree_function'])\n",
    "            age = float(request.json['age'])\n",
    "\n",
    "            # Loading the saved models into memory\n",
    "            filename_scaler = 'scaler_model.pickle'\n",
    "            filename = 'xgboost_model.pickle'\n",
    "            scaler_model = pickle.load(open(filename_scaler, 'rb'))\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "            # predictions using the loaded model file\n",
    "            scaled_data=scaler_model.transform([[number_of_times_pregnant,plasma_glucose_concentration,diastolic_blood_pressure,triceps_skinfold_thickness,serum_insulin,body_mass_index,diabetes_pedigree_function, age]])\n",
    "            prediction=loaded_model.predict(scaled_data)\n",
    "            print('prediction is', prediction[0])\n",
    "            if prediction[0]==1:\n",
    "                result= 'The Patient is Diabetic'\n",
    "            else:\n",
    "                result = 'The Patient is not Diabetic'\n",
    "            # showing the prediction results in a UI\n",
    "            return jsonify(result)\n",
    "        except Exception as e:\n",
    "            print('The Exception message is: ',e)\n",
    "            return jsonify('error: Something is wrong')\n",
    "    # return render_template('results.html')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #app.run(host='127.0.0.1', port=8001, debug=True)\n",
    "   app.run(debug=True) # running the app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment to G-cloud:\n",
    "* Go to https://cloud.google.com/ and create an account if already haven’t created one. Then go to the console of your account.\n",
    "* Go to IAM and admin(highlighted) and click manage resources.\n",
    "  <img src=\"manage_resources.PNG\" >\n",
    "            \n",
    "* Click _CREATE PROJECT_ to create a new project for deployment.\n",
    "* Once the project gets created, select _App Engine_ and select _Dashboard_.\n",
    "    <img src=\"dashboard.PNG\" >\n",
    "   \n",
    "* Go to https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe to download the google cloud SDK to your machine.\n",
    "* Click _Start Tutorial_ on the screen and select _Python app_ and click start.\n",
    "<img src=\"get_started.PNG\" width=\"300\" >\n",
    "* Check whether the correct project name is displayed and then click next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a file **app.yaml** and put  _runtime: python37_ in that file as shown\n",
    "<img src=\"app_yaml.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a **requirements.txt** file by opening the command prompt/anaconda prompt, navigate to the project folder and enter the command **pip freeze > requirements.txt**\n",
    "\n",
    "It is recommended to use separate environments for different projects.\n",
    "\n",
    "The contents of **requirements.txt** are:\n",
    "\n",
    "certifi==2019.11.28\n",
    "\n",
    "Click==7.0\n",
    "\n",
    "Flask==1.1.1\n",
    "\n",
    "Flask-Cors==3.0.8\n",
    "\n",
    "itsdangerous==1.1.0\n",
    "\n",
    "Jinja2==2.10.3\n",
    "\n",
    "joblib==0.14.1\n",
    "\n",
    "MarkupSafe==1.1.1\n",
    "\n",
    "numpy==1.18.0\n",
    "\n",
    "scikit-learn==0.22.1\n",
    "\n",
    "scipy==1.4.1\n",
    "\n",
    "six==1.13.0\n",
    "\n",
    "Werkzeug==0.16.0\n",
    "\n",
    "wincertstore==0.2\n",
    "\n",
    "xgboost==0.90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Your python application file should be called ‘main.py’. It is a GCP specific requirement.\n",
    "* Open gcloud shell , navigate to the project folder and enter the command gcloud init to initialise the gcloud context.\n",
    "* It asks you to select from the list of available projects.\n",
    "\n",
    "<img src=\"available_projects.PNG\" width=\"300\">\n",
    "\n",
    "* Once the project name is selected, enter the command **gcloud app deploy app.yaml --project (project name)**\n",
    "* After executing the above command, GCP will ask you to enter the region for your application. Choose the appropriate one.\n",
    "<img src=\"region_selection.PNG\" width=\"300\">\n",
    "\n",
    "* GCP will ask for the services to be deployed. Enter ‘y’ to deploy the services.\n",
    "<img src=\"yes.PNG\" width=\"300\">\n",
    "\n",
    "* And then it will give you the link for your app.\n",
    "* To save money, go to settings and disable your app.\n",
    "<img src=\"disable.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The final app can be accessed using _Postman_ as shown below:\n",
    "Final app screenshot:\n",
    "<img src=\"final.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
